knitr::opts_chunk$set(echo = TRUE)
# Only set root.dir if we are actually knitting a file (not running interactively)
input <- knitr::current_input()
if (!is.null(input)) {
knitr::opts_knit$set(root.dir = dirname(input))
}
# Ensure preamble.tex exists right next to this Rmd (safety net)
if (!file.exists("preamble.tex")) {
writeLines(c(
"\\usepackage{textcomp}",
"\\providecommand{\\textquoteright}{'}",
"\\providecommand{\\textquoteleft}{`}",
"\\providecommand{\\textquotedblleft}{``}",
"\\providecommand{\\textquotedblright}{''}"
), "preamble.tex")
}
# Sanitize curly quotes and stray latex macros in all printed output
hook_out <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
x <- gsub("\u2019|\u2018", "'", x, useBytes = TRUE)      # ’ ‘ → '
x <- gsub("\u201C|\u201D", "\"", x, useBytes = TRUE)     # “ ” → "
x <- gsub("\\\\textquoteright", "'", x, perl = TRUE)     # latex macro → '
if (is.function(hook_out)) hook_out(x, options) else x
})
knitr::opts_chunk$set(echo = TRUE)
# Only set root.dir when knitting a file
input <- knitr::current_input()
if (!is.null(input)) {
knitr::opts_knit$set(root.dir = dirname(input))
}
# Ensure preamble exists next to the Rmd (belt & suspenders)
if (!file.exists("preamble.tex")) {
writeLines(c(
"\\usepackage{textcomp}",
"\\providecommand{\\textquoteleft}{`}",
"\\providecommand{\\textquoteright}{'}",
"\\providecommand{\\textquotedblleft}{``}",
"\\providecommand{\\textquotedblright}{''}",
"\\providecommand{\\ae}{æ}",
"\\providecommand{\\AE}{Æ}",
"\\providecommand{\\dh}{ð}",
"\\providecommand{\\DH}{Ð}",
"\\providecommand{\\th}{þ}",
"\\providecommand{\\TH}{Þ}",
"\\providecommand{\\o}{ø}",
"\\providecommand{\\O}{Ø}",
"\\providecommand{\\ss}{ß}"
), "preamble.tex")
}
# Sanitize printed output from chunks (tibbles, scraped text, etc.)
hook_out <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
# Smart quotes → ASCII
x <- gsub("\u2019|\u2018", "'", x, useBytes = TRUE)
x <- gsub("\u201C|\u201D", "\"", x, useBytes = TRUE)
# Pandoc macro that sometimes leaks
x <- gsub("\\\\textquoteright", "'", x, perl = TRUE)
# Legacy LaTeX letter macros → Unicode
x <- gsub("\\\\ae", "æ", x, perl = TRUE)
x <- gsub("\\\\AE", "Æ", x, perl = TRUE)
x <- gsub("\\\\dh", "ð", x, perl = TRUE)
x <- gsub("\\\\DH", "Ð", x, perl = TRUE)
x <- gsub("\\\\th", "þ", x, perl = TRUE)
x <- gsub("\\\\TH", "Þ", x, perl = TRUE)
x <- gsub("\\\\o(?![a-zA-Z])", "ø", x, perl = TRUE)  # avoid \over etc.
x <- gsub("\\\\O(?![a-zA-Z])", "Ø", x, perl = TRUE)
x <- gsub("\\\\ss", "ß", x, perl = TRUE)
if (is.function(hook_out)) hook_out(x, options) else x
})
knitr::opts_chunk$set(echo = TRUE)
# Only set root.dir when knitting a file
input <- knitr::current_input()
if (!is.null(input)) {
knitr::opts_knit$set(root.dir = dirname(input))
}
# Ensure preamble exists next to the Rmd (belt & suspenders)
if (!file.exists("preamble.tex")) {
writeLines(c(
"\\usepackage{textcomp}",
"\\providecommand{\\textquoteleft}{`}",
"\\providecommand{\\textquoteright}{'}",
"\\providecommand{\\textquotedblleft}{``}",
"\\providecommand{\\textquotedblright}{''}",
"\\providecommand{\\ae}{æ}",
"\\providecommand{\\AE}{Æ}",
"\\providecommand{\\dh}{ð}",
"\\providecommand{\\DH}{Ð}",
"\\providecommand{\\th}{þ}",
"\\providecommand{\\TH}{Þ}",
"\\providecommand{\\o}{ø}",
"\\providecommand{\\O}{Ø}",
"\\providecommand{\\ss}{ß}"
), "preamble.tex")
}
# Sanitize printed output from chunks (tibbles, scraped text, etc.)
hook_out <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
# Smart quotes → ASCII
x <- gsub("\u2019|\u2018", "'", x, useBytes = TRUE)
x <- gsub("\u201C|\u201D", "\"", x, useBytes = TRUE)
# Pandoc macro that sometimes leaks
x <- gsub("\\\\textquoteright", "'", x, perl = TRUE)
# Legacy LaTeX letter macros → Unicode
x <- gsub("\\\\ae", "æ", x, perl = TRUE)
x <- gsub("\\\\AE", "Æ", x, perl = TRUE)
x <- gsub("\\\\dh", "ð", x, perl = TRUE)
x <- gsub("\\\\DH", "Ð", x, perl = TRUE)
x <- gsub("\\\\th", "þ", x, perl = TRUE)
x <- gsub("\\\\TH", "Þ", x, perl = TRUE)
x <- gsub("\\\\o(?![a-zA-Z])", "ø", x, perl = TRUE)  # avoid \over etc.
x <- gsub("\\\\O(?![a-zA-Z])", "Ø", x, perl = TRUE)
x <- gsub("\\\\ss", "ß", x, perl = TRUE)
if (is.function(hook_out)) hook_out(x, options) else x
})
library(gt)
library(tibble)
library(dplyr)
md_col <- function(tbl, col) {
# Safely convert a single column's markdown to gt::md()
col <- rlang::ensym(col)
tbl %>% mutate(!!col := gt::md(!!col))
}
# --- Quanteda ---
quanteda_tbl <- tibble(
Task = c(
"Convert dfm to tibble",
"Get token list",
"Combine with metadata",
"Tidy version"
),
`What to do` = c(
"`dfm %>% convert(to = \"data.frame\") %>% as_tibble()`",
"`tokens %>% as.list()`",
"`docvars(dfm)` gives doc-level covariates",
"`Use quanteda.textmodels::convert()` or `tidytext::tidy()` if applicable"
)
)
quanteda_tbl %>%
md_col(`What to do`) %>%
gt() %>%
tab_header(title = "Quanteda (tokens, dfm, etc.)") %>%
cols_label(`What to do` = "What to do")
# --- tm ---
tm_tbl <- tibble(
Task = c(
"Convert Corpus",
"DTM to tidy format",
"Combine with meta"
),
`What to do` = c(
"`sapply(corpus, as.character)` or `content(corpus)`",
"`tidy(DTM)` from `tidytext`",
"`meta(corpus)` or `tm_map()`"
)
)
tm_tbl %>%
md_col(`What to do`) %>%
gt() %>%
tab_header(title = "tm (Corpus, DocumentTermMatrix)") %>%
cols_label(`What to do` = "What to do")
# --- text2vec ---
text2vec_tbl <- tibble(
Task = c(
"Convert DTM",
"Vocabulary to tibble",
"Embeddings to tibble"
),
`What to do` = c(
"`as.matrix(dtm) %>% as_tibble()` or `Matrix::summary(dtm)`",
"`vocab$term_stats %>% as_tibble()`",
"`as_tibble(embeddings)`"
)
)
text2vec_tbl %>%
md_col(`What to do`) %>%
gt() %>%
tab_header(title = "text2vec (itoken, dtm, etc.)") %>%
cols_label(`What to do` = "What to do")
# --- topicmodels / stm ---
stm_tbl <- tibble(
Task = c(
"Tidy topics + terms",
"Tidy docs + topics",
"STM effects"
),
`What to do` = c(
"`tidy(model, matrix = \"beta\")` (from `broom`)",
"`tidy(model, matrix = \"gamma\")`",
"`Use estimateEffect()` then extract manually"
)
)
stm_tbl %>%
md_col(`What to do`) %>%
gt() %>%
tab_header(title = "topicmodels / stm (LDA/STMs)") %>%
cols_label(`What to do` = "What to do")
# --- Sparse matrices ---
sparse_tbl <- tibble(
Task = c(
"Convert to dense",
"Extract non-zeros",
"Tidy-friendly format"
),
`What to do` = c(
"`as.matrix(sparse_mat)`",
"`Matrix::summary(sparse_mat)`",
"`Use reshape2::melt()` or `pivot_longer()`"
)
)
sparse_tbl %>%
md_col(`What to do`) %>%
gt() %>%
tab_header(title = "Sparse matrices (e.g., dgCMatrix)") %>%
cols_label(`What to do` = "What to do")
# --- Tidyverse Tools ---
tidyverse_tools_tbl <- tibble(
`Tool/Package` = c(
"tidytext",
"textrecipes",
"tidylo, textdata",
"unnest_tokens()",
"broom / broom.mixed"
),
Use = c(
"Makes many text objects tidy (DTMs, LDA, etc.)",
"Tidy preprocessing (e.g., tokenization, tf-idf)",
"Additional tidy tools for text and lexicons",
"From `tidytext`, best for converting raw text to tokens",
"Tidy output from models (including `topicmodels`)"
)
)
tidyverse_tools_tbl %>%
md_col(Use) %>%
gt() %>%
tab_header(title = "Helpful Tidyverse-Compatible Tools")
library(tidyverse)
census <- tibble(
name = c("Mary Jenkins", "Samuel Price", "Hannah Lee", "Charles Morton", "Unnamed Child"),
occupation = c("washerwoman", NA, "seamstress", "laborer", NA),
age = c(32, 44, 29, 51, 1))
cenus
census
census %>%
mutate(occupation = str_detect(conservation, "washerwoman"))
census %>%
mutate(is_washerwoman = str_detect(occupation, "washerwoman"))
census %>%
mutate(is_missing = is.na(occupation))
library(tidyverse)
# Replace NA in ""conservation" with the string "missing"
census <- census %>%
mutate(occupation = replace_na(occupation, "missing"))
# View the updated data
census %>%
slice(1:10)
knitr::opts_chunk$set(echo = TRUE)
# Only set root.dir when knitting a file
input <- knitr::current_input()
if (!is.null(input)) {
knitr::opts_knit$set(root.dir = dirname(input)) }
# Ensure preamble exists next to the Rmd (belt & suspenders)
if (!file.exists("preamble.tex")) {
writeLines(c(
"\\usepackage{textcomp}",
"\\providecommand{\\textquoteleft}{`}",
"\\providecommand{\\textquoteright}{'}",
"\\providecommand{\\textquotedblleft}{``}",
"\\providecommand{\\textquotedblright}{''}",
"\\providecommand{\\ae}{æ}",
"\\providecommand{\\AE}{Æ}",
"\\providecommand{\\dh}{ð}",
"\\providecommand{\\DH}{Ð}",
"\\providecommand{\\th}{þ}",
"\\providecommand{\\TH}{Þ}",
"\\providecommand{\\o}{ø}",
"\\providecommand{\\O}{Ø}",
"\\providecommand{\\ss}{ß}"
), "preamble.tex")
}
# Sanitize printed output from chunks (tibbles, scraped text, etc.)
hook_out <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
# Smart quotes → ASCII
x <- gsub("\u2019|\u2018", "'", x, useBytes = TRUE)
x <- gsub("\u201C|\u201D", "\"", x, useBytes = TRUE)
# Pandoc macro that sometimes leaks
x <- gsub("\\\\textquoteright", "'", x, perl = TRUE)
# Legacy LaTeX letter macros → Unicode
x <- gsub("\\\\ae", "æ", x, perl = TRUE)
x <- gsub("\\\\AE", "Æ", x, perl = TRUE)
x <- gsub("\\\\dh", "ð", x, perl = TRUE)
x <- gsub("\\\\DH", "Ð", x, perl = TRUE)
x <- gsub("\\\\th", "þ", x, perl = TRUE)
x <- gsub("\\\\TH", "Þ", x, perl = TRUE)
x <- gsub("\\\\o(?![a-zA-Z])", "ø", x, perl = TRUE)  # avoid \over etc.
x <- gsub("\\\\O(?![a-zA-Z])", "Ø", x, perl = TRUE)
x <- gsub("\\\\ss", "ß", x, perl = TRUE)
if (is.function(hook_out)) hook_out(x, options) else x
})
library("tidyverse")
library("httr")
library("jsonlite")
# Define the JSON API endpoint
url <- "https://www.dallasopendata.com/resource/q3v5-8c56.json"
# Make GET request to extract JSON content
response <- GET(url)
# If accessing the server is successful it returns the code 200
if (status_code(response) == 200) {
# Extract and parse JSON content into a data frame
json_content <- content(response, "text", encoding = "UTF-8")
animal_shelter_data <- fromJSON(json_content, flatten = TRUE) %>%
as_tibble() } else {
stop("Failed to retrieve data. Status code: ", status_code(response)) }
library("tidyverse")
library("httr")
library("jsonlite")
# Define the JSON API endpoint
url <- "https://www.dallasopendata.com/api/v3/views/2ksy-mdcf/query.json"
# Make GET request to extract JSON content
response <- GET(url)
# If accessing the server is successful it returns the code 200
if (status_code(response) == 200) {
# Extract and parse JSON content into a data frame
json_content <- content(response, "text", encoding = "UTF-8")
animal_shelter_data <- fromJSON(json_content, flatten = TRUE) %>%
as_tibble() } else {
stop("Failed to retrieve data. Status code: ", status_code(response)) }
library("tidyverse")
library("httr")
library("jsonlite")
# Define the JSON API endpoint
url <- "https://www.dallasopendata.com/resource/q3v5-8c56.json"
# Make GET request to extract JSON content
response <- GET(url)
# If accessing the server is successful it returns the code 200
if (status_code(response) == 200) {
# Extract and parse JSON content into a data frame
json_content <- content(response, "text", encoding = "UTF-8")
animal_shelter_data <- fromJSON(json_content, flatten = TRUE) %>%
as_tibble() } else {
stop("Failed to retrieve data. Status code: ", status_code(response)) }
library("tidyverse")
library("httr")
library("jsonlite")
# Define the JSON API endpoint
url <- "https://data.cityofnewyork.us/resource/xywu-7bv9.json"
# Make GET request to extract JSON content
response <- GET(url)
# If accessing the server is successful it returns the code 200
if (status_code(response) == 200) {
# Extract and parse JSON content into a data frame
json_content <- content(response, "text", encoding = "UTF-8")
animal_shelter_data <- fromJSON(json_content, flatten = TRUE) %>%
as_tibble() } else {
stop("Failed to retrieve data. Status code: ", status_code(response)) }
# Show just the first five columns
head(animal_shelter_data[, 1:5])
library("tidyverse")
library("httr")
library("jsonlite")
# Define the JSON API endpoint
url <- "https://www.dallasopendata.com/resource/shgm-yzbp.json"
# Make GET request to extract JSON content
response <- GET(url)
# If accessing the server is successful it returns the code 200
if (status_code(response) == 200) {
# Extract and parse JSON content into a data frame
json_content <- content(response, "text", encoding = "UTF-8")
animal_shelter_data <- fromJSON(json_content, flatten = TRUE) %>%
as_tibble() } else {
stop("Failed to retrieve data. Status code: ", status_code(response)) }
library("tidyverse")
library("httr")
library("jsonlite")
# Define the JSON API endpoint
url <- "https://www.dallasopendata.com/resource/shgm-yzbp.json"
# Make GET request to extract JSON content
response <- GET(url)
# If accessing the server is successful it returns the code 200
if (status_code(response) == 200) {
# Extract and parse JSON content into a data frame
json_content <- content(response, "text", encoding = "UTF-8")
service_requests <- fromJSON(json_content, flatten = TRUE) %>%
as_tibble() } else {
stop("Failed to retrieve data. Status code: ", status_code(response)) }
# Show just the first five columns
head(service_requests[, 1:5])
library("tidyverse")
library("httr")
library("jsonlite")
# Define the JSON API endpoint
url <- "https://data.cityofnewyork.us/resource/wewp-mm3p.json"
# Make GET request to extract JSON content
response <- GET(url)
# If accessing the server is successful it returns the code 200
if (status_code(response) == 200) {
# Extract and parse JSON content into a data frame
json_content <- content(response, "text", encoding = "UTF-8")
service_requests <- fromJSON(json_content, flatten = TRUE) %>%
as_tibble() } else {
stop("Failed to retrieve data. Status code: ", status_code(response)) }
# Show just the first five columns
head(service_requests[, 1:5])
json_list <- fromJSON(json_content)
subset_list <- json_list[1:2, ]
cat(prettify(toJSON(subset_list, auto_unbox = TRUE)))
nrow(json_list)
library("tidyverse")
library("httr")
library("jsonlite")
# Define the JSON API endpoint
url <- "https://data.cityofnewyork.us/resource/wewp-mm3p.json"
# Make GET request to extract JSON content
response <- GET(url)
# If accessing the server is successful it returns the code 200
if (status_code(response) == 200) {
# Extract and parse JSON content into a data frame
json_content <- content(response, "text", encoding = "UTF-8")
service_requests <- fromJSON(json_content, flatten = TRUE) %>%
as_tibble() } else {
stop("Failed to retrieve data. Status code: ", status_code(response)) }
# Show just the first five columns
head(service_requests[, 1:5])
json_list <- fromJSON(json_content)
subset_list <- json_list[1:2, ]
cat(prettify(toJSON(subset_list, auto_unbox = TRUE)))
nrow(json_list)
url <- "https://data.cityofnewyork.us/resource/wewp-mm3p.json"
# Pagination settings
limit <- 1000
# Will store the output from each iteration
animal_shelter_data <- tibble()
# Run for two iterations: first with offset = 0, then with offset = 1000.
# This means the code will first collect rows 1–1000,
# and then collect rows 1001–2000 on the second iteration.
for (i in 0:1) {
offset <- i * limit
# Make paginated request
response <- GET(url, query = list(`$limit` = limit, `$offset` = offset))
if (status_code(response) != 200) {
stop("Request failed. Status code: ", status_code(response)) }
# Parse the response
data_subset <- fromJSON(content(response, "text"), flatten = TRUE)
# Save collected data
animal_shelter_data <- bind_rows(animal_shelter_data, as_tibble(data_subset)) }
knitr::opts_chunk$set(echo = TRUE)
# Only set root.dir when knitting a file
input <- knitr::current_input()
if (!is.null(input)) {
knitr::opts_knit$set(root.dir = dirname(input)) }
# Ensure preamble exists next to the Rmd (belt & suspenders)
if (!file.exists("preamble.tex")) {
writeLines(c(
"\\usepackage{textcomp}",
"\\providecommand{\\textquoteleft}{`}",
"\\providecommand{\\textquoteright}{'}",
"\\providecommand{\\textquotedblleft}{``}",
"\\providecommand{\\textquotedblright}{''}",
"\\providecommand{\\ae}{æ}",
"\\providecommand{\\AE}{Æ}",
"\\providecommand{\\dh}{ð}",
"\\providecommand{\\DH}{Ð}",
"\\providecommand{\\th}{þ}",
"\\providecommand{\\TH}{Þ}",
"\\providecommand{\\o}{ø}",
"\\providecommand{\\O}{Ø}",
"\\providecommand{\\ss}{ß}"
), "preamble.tex")
}
# Sanitize printed output from chunks (tibbles, scraped text, etc.)
hook_out <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
# Smart quotes → ASCII
x <- gsub("\u2019|\u2018", "'", x, useBytes = TRUE)
x <- gsub("\u201C|\u201D", "\"", x, useBytes = TRUE)
# Pandoc macro that sometimes leaks
x <- gsub("\\\\textquoteright", "'", x, perl = TRUE)
# Legacy LaTeX letter macros → Unicode
x <- gsub("\\\\ae", "æ", x, perl = TRUE)
x <- gsub("\\\\AE", "Æ", x, perl = TRUE)
x <- gsub("\\\\dh", "ð", x, perl = TRUE)
x <- gsub("\\\\DH", "Ð", x, perl = TRUE)
x <- gsub("\\\\th", "þ", x, perl = TRUE)
x <- gsub("\\\\TH", "Þ", x, perl = TRUE)
x <- gsub("\\\\o(?![a-zA-Z])", "ø", x, perl = TRUE)  # avoid \over etc.
x <- gsub("\\\\O(?![a-zA-Z])", "Ø", x, perl = TRUE)
x <- gsub("\\\\ss", "ß", x, perl = TRUE)
if (is.function(hook_out)) hook_out(x, options) else x
})
library(rvest)
# Step 1: Read the webpage
url <- "https://example.com"
page <- read_html(url)
# Step 2: Select and extract all <p> tags using a CSS selector
paragraphs <- page %>%
html_elements("p") %>%      # CSS selector
html_text(trim = TRUE)      # extract clean text
paragraphs
library(rvest)
# Step 1: Read the webpage
url <- "https://example.com"
page <- read_html(url)
# Step 2: Select and extract all <p> tags using a CSS selector
paragraphs <- page %>%
html_elements("p") %>%      # CSS selector
html_text(trim = TRUE)      # extract clean text
print(paragraphs)
# Extract <p> tags nested inside <div class='content'>
paragraphs <- page %>%
html_elements(xpath = "//div[@class='content']/p") %>%
html_text(trim = TRUE)
print(paragraphs)
